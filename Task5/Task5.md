# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Тихонов Егор Станиславович
- РИ-220947  

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.


## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.


В первом скрипте переменная distanceToTarget отображает текущую взаимосвязь между Agent и Target. Барьером является коэффициент 1.42
![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185026149101871144/image.png?ex=658e1c9a&is=657ba79a&hm=32291e2b90004cd752acf870a92176c3f3c8fed3b0160de79ec1e57c700dccbb&=&format=webp&quality=lossless&width=1148&height=276 "Коэффицент корреляции")

Для второго случая коэффициентом является процент изменения цены. Барьером является 6 процентов
![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185026149462585435/image.png?ex=658e1c9a&is=657ba79a&hm=d104bd110c8e435917ab23d10897b4f1cf3cdb38212add968b42eb5f75412ed7&=&format=webp&quality=lossless&width=1081&height=443 "Коэффицент корреляции")


В данном случае влияние коэффициента корреляции на обучение выглядит следующим образом: Чем он меньше, тем точнее будет обучаться модель, так как он служит жесткой верхней границей, определяющей вознаграждение. Однако важно отметить, что уменьшение этого параметра может значительно увеличить время обучения модели.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

#### 1) play_against_latest_model_ratio - Этот параметр определяет долю игр, в которых агент соревнуется с последней версией самого себя во время обучения.Более высокий коэффициент означает, что агенту часто бросают вызов его последней и, предположительно, лучшей стратегии, которая может ускорить обучение. Однако высокое соотношение может также привести к переоснащению его собственных стратегий, снижая способность агента обобщать действия на новых противников.

Значение 0.5:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226540851260/image.png?ex=658e29b6&is=657bb4b6&hm=19b78ce9e4451bb441bc916fb5ffcc78ca0fe7ef0bb3e47b657749863e7ef053&=&format=webp&quality=lossless&width=875&height=662 "Environment, play_against_latest_model_ratio = 0.5")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226213711872/image.png?ex=658e29b6&is=657bb4b6&hm=87a9db9647bd3fa82ea9b875f04c626000c70b1c3e0ae22d00d2a78411d1be13&=&format=webp&quality=lossless&width=1251&height=662 "Policy, play_against_latest_model_ratio = 0.5")

---
Значение 1:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185042620305653831/image.png?ex=658e2bf1&is=657bb6f1&hm=6d6a69b8b572a8976b3a6f00585eda6558787079b010de3512f8a2c6209210d2&=&format=webp&quality=lossless&width=1336&height=661 "Environment, play_against_latest_model_ratio = 1.0")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185042620767010896/image.png?ex=658e2bf1&is=657bb6f1&hm=ed3507b43167f6c2d5736fb532df550e6a46073310ca007c2e2f0294ab84471d&=&format=webp&quality=lossless&width=1440&height=657 "Policy, play_against_latest_model_ratio = 1.0")

При значении 1 график назначения награды ведёт себя крайне нестабильно, так как агент постоянно пытается менять ход своих действий.


#### 2) extrinsic -> strength - отвечает за вознаграждение, которое получает агент. Более высокое значение силы означает, что агент получает более крупные награды, что может стимулировать более агрессивное исследование окружающей среды. Слишком высокое значение может привести к тому, что агент сосредоточится на краткосрочных вознаграждениях. Низкое значение будет стимулировать более консервативное иследование, но также замедлит процесс обучения

Значение 1:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226213711872/image.png?ex=658e29b6&is=657bb4b6&hm=87a9db9647bd3fa82ea9b875f04c626000c70b1c3e0ae22d00d2a78411d1be13&=&format=webp&quality=lossless&width=1251&height=662 "Environment, extrinsic -> strength = 1")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226540851260/image.png?ex=658e29b6&is=657bb4b6&hm=19b78ce9e4451bb441bc916fb5ffcc78ca0fe7ef0bb3e47b657749863e7ef053&=&format=webp&quality=lossless&width=875&height=662 "Policy, extrinsic -> strength = 1")

---

Значение 10.0:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185049579104776192/image.png?ex=658e326c&is=657bbd6c&hm=b2e34190a1ff25d9541a4b766f74d52666aa981436e20a6909cbb4b28b5d6b3e&=&format=webp&quality=lossless&width=1245&height=662 "Environment, extrinsic -> strength = 10.0")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185049579503243374/image.png?ex=658e326c&is=657bbd6c&hm=033f26fa9019f67e92c007a8a221deb37297fa459bfe30a93c0c166fca0419e7&=&format=webp&quality=lossless&width=1362&height=661 "Policy, extrinsic -> strength = 10.0")

При внутренней силе награды = 1,внешняя награда строго равна коэффициенту кратности. На графиках видно, что график неудачных попыток при высоком значении коэффициента награды приобретает скачущую природу, хоть и сохраняет тенденцию на снижение с увеличением количества шагов обучения. Значит слишком высокие значения приводят к нестабильному процессу обучения. 

#### 3) gamma - это коэффициент, используемый при расчете будущих вознаграждений. Он определяет, насколько большое значение мы хотим придать будущим вознаграждениям.

При высоком значении (близком к 1) агент будет учитывать будущие вознаграждения с большим весом, то есть он будет больше сосредоточен на долгосрочных выгодах. При низком значении (близком к 0) агент будет больше ориентироваться на сиюминутное вознаграждение.

В контексте игры, если гамма установлена на высокое значение, агент будет отдавать предпочтение действиям, которые могут не иметь немедленного вознаграждения, но приведут к большему вознаграждению в будущем. И наоборот, если гамма установлена на низкое значение, агент будет отдавать предпочтение действиям, которые дают немедленное вознаграждение.

Значение 0.01:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185058039506681877/image.png?ex=658e3a4d&is=657bc54d&hm=8c5aafa574529acb755be24564aa54a43679d8c3bb2c745f65168d61f2f997d4&=&format=webp&quality=lossless&width=1308&height=661 "Environment, extrinsic -> gamma = 0.01")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185058039909322843/image.png?ex=658e3a4e&is=657bc54e&hm=29058a36846e6c84013ab4b56ebb4d8605a315503f84c895716531e7350c5e81&=&format=webp&quality=lossless&width=1365&height=662 "Policy, extrinsic -> gamma = 0.01")

---

Значение 0.99:

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226540851260/image.png?ex=658e29b6&is=657bb4b6&hm=19b78ce9e4451bb441bc916fb5ffcc78ca0fe7ef0bb3e47b657749863e7ef053&=&format=webp&quality=lossless&width=875&height=662 "Environment, extrinsic -> gamma = 0.99")

![Alt text](https://media.discordapp.net/attachments/914593555526320151/1185040226213711872/image.png?ex=658e29b6&is=657bb4b6&hm=87a9db9647bd3fa82ea9b875f04c626000c70b1c3e0ae22d00d2a78411d1be13&=&format=webp&quality=lossless&width=1251&height=662 "Policy, extrinsic -> gamma = 0.99")


## Задание 3
### Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Пример 1 (Ищущий Агент): Этот тип агента можно использовать в стелс-играх, где NPC должен искать игрока, или в гоночных играх,где ИИ-противнику необходимо искать финишную черту.

Пример 2 (От шахты к продаже): Этот тип агента можно использовать в играх, где NPC должен передвигаться по строгому маршруту, например в стратегической игре, где он также должен ходить от шахты с ресурсами до главного здания.

Использование ML-агента может оказаться проще, чем программирование решения, в тех случаях, когда среда сложна и динамична, а агенту необходимо адаптироваться к меняющимся условиям. Например, в игре, где расположение карты меняется каждый раз, когда вы играете, было бы сложно запрограммировать решение, которое работало бы для каждой возможной карты.
Однако, если задача проста и среда статична, возможно, будет проще запрограммировать решение. Например, если NPC просто нужно следовать фиксированному пути, было бы проще запрограммировать этот путь, чем обучать ML-агента его изучению.


## Выводы
В ходе работы я ознакомился с применением  Unity ML-Agent'ов в разработке игр. Также были построены графики различных параметров нейронной сети с помощью tensorflow


Цели лабораторной работы были достигнуты.
## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
